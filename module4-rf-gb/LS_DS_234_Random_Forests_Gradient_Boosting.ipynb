{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lambda School Data Science — Classification & Validation_ \n",
    "\n",
    "# Random Forests & Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting and Random Forest are often the best choice for “Spreadsheet Machine Learning.”\n",
    "- Meaning, [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) for supervised learning with structured, tabular data.\n",
    "- Because trees can fit non-linear, non-monotonic relationships, and interactions between features.\n",
    "- A single decision tree, grown to unlimited depth, will overfit. We solve this problem by ensembling trees, with bagging or boosting.\n",
    "- One-hot encoding isn’t the only way, and may not be the best way, of categorical encoding for tree ensembles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "#### Decision Trees\n",
    "- A Visual Introduction to Machine Learning, [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/),  and [Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)\n",
    "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n",
    "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
    "- [How a Russian mathematician constructed a decision tree - by hand - to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
    "- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU)\n",
    "\n",
    "#### Random Forests\n",
    "- [Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)\n",
    "- [Coloring with Random Forests](http://structuringtheunstructured.blogspot.com/2017/11/coloring-with-random-forests.html)\n",
    "- [Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "\n",
    "#### Gradient Boosting\n",
    "- [A Gentle Introduction to the Gradient Boosting Algorithm for Machine Learning](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)\n",
    "- [A Kaggle Master Explains Gradient Boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)\n",
    "- [How to explain gradient boosting](https://explained.ai/gradient-boosting/index.html)\n",
    "\n",
    "#### Python libraries for Gradient Boosting\n",
    "- [scikit-learn Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting) — slower than other libraries, but [the next version may be better](https://twitter.com/amuellerml/status/1123613520426426368)\n",
    "  - Anaconda: already installed\n",
    "  - Google Colab: already installed\n",
    "- [xgboost](https://xgboost.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://xiaoxiaowang87.github.io/monotonicity_constraint/)\n",
    "  - Anaconda, Mac/Linux: `conda install -c conda-forge xgboost`\n",
    "  - Windows: `pip install xgboost`\n",
    "  - Google Colab: already installed\n",
    "- [LightGBM](https://lightgbm.readthedocs.io/en/latest/) — can accept missing values and enforce [monotonic constraints](https://blog.datadive.net/monotonicity-constraints-in-machine-learning/)\n",
    "  - Anaconda: `conda install -c conda-forge lightgbm`\n",
    "  - Google Colab: already installed\n",
    "- [CatBoost](https://catboost.ai/) — can accept missing values and use [categorical features](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html) without preprocessing\n",
    "  - Anaconda: `conda install -c conda-forge catboost`\n",
    "  - Google Colab: `pip install catboost`\n",
    "\n",
    "#### Categorical encoding for trees\n",
    "- [Are categorical variables getting lost in your random forests?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)\n",
    "- [Beyond One-Hot: An Exploration of Categorical Variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)\n",
    "- [Categorical Features and Encoding in Decision Trees](https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)\n",
    "- [Coursera — How to Win a Data Science Competition: Learn from Top Kagglers — Concept of mean encoding](https://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv)\n",
    "- [Mean (likelihood) encodings: a comprehensive study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)\n",
    "- [The Mechanics of Machine Learning, Chapter 6: Categorically Speaking](https://mlbook.explained.ai/catvars.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golf Putts (regression, 1 feature, non-linear)\n",
    "\n",
    "https://statmodeling.stat.columbia.edu/2008/12/04/the_golf_puttin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "putts = pd.DataFrame(\n",
    "    columns=['distance', 'tries', 'successes'], \n",
    "    data = [[2, 1443, 1346],\n",
    "            [3, 694, 577],\n",
    "            [4, 455, 337],\n",
    "            [5, 353, 208],\n",
    "            [6, 272, 149],\n",
    "            [7, 256, 136],\n",
    "            [8, 240, 111],\n",
    "            [9, 217, 69],\n",
    "            [10, 200, 67],\n",
    "            [11, 237, 75],\n",
    "            [12, 202, 52],\n",
    "            [13, 192, 46],\n",
    "            [14, 174, 54],\n",
    "            [15, 167, 28],\n",
    "            [16, 201, 27],\n",
    "            [17, 195, 31],\n",
    "            [18, 191, 33],\n",
    "            [19, 147, 20],\n",
    "            [20, 152, 24]]\n",
    ")\n",
    "\n",
    "putts['rate of success'] = putts['successes'] / putts['tries']\n",
    "putts_X = putts[['distance']]\n",
    "putts_y = putts['rate of success']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docs\n",
    "- [Scikit-Learn User Guide: Random Forests](https://scikit-learn.org/stable/modules/ensemble.html#random-forests) (`from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier`)\n",
    "- [XGBoost Python API Reference: Scikit-Learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) (`from xgboost import XGBRegressor, XGBClassifier`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def putt_trees(max_depth=1, n_estimators=1):\n",
    "    models = [DecisionTreeRegressor(max_depth=max_depth), \n",
    "              RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators), \n",
    "              XGBRegressor(max_depth=max_depth, n_estimators=n_estimators)]\n",
    "    \n",
    "    for model in models:\n",
    "        name = model.__class__.__name__\n",
    "        model.fit(putts_X, putts_y)\n",
    "        ax = putts.plot('distance', 'rate of success', kind='scatter', title=name)\n",
    "        ax.step(putts_X, model.predict(putts_X), where='mid')\n",
    "        plt.show()\n",
    "        \n",
    "interact(putt_trees, max_depth=(1,6,1), n_estimators=(10,40,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Do-it-yourself Bagging Ensemble of Decision Trees (like a Random Forest)\n",
    "def diy_bagging(max_depth=1, n_estimators=1):\n",
    "    y_preds = []\n",
    "    for i in range(n_estimators):\n",
    "        title = f'Tree {i+1}'\n",
    "        bootstrap_sample = putts.sample(n=len(putts), replace=True).sort_values(by='distance')\n",
    "        bootstrap_X = bootstrap_sample[['distance']]\n",
    "        bootstrap_y = bootstrap_sample['rate of success']\n",
    "        tree = DecisionTreeRegressor(max_depth=max_depth)\n",
    "        tree.fit(bootstrap_X, bootstrap_y)\n",
    "        y_pred = tree.predict(bootstrap_X)\n",
    "        y_preds.append(y_pred)\n",
    "        ax = bootstrap_sample.plot('distance', 'rate of success', kind='scatter', title=title)\n",
    "        ax.step(bootstrap_X, y_pred, where='mid')\n",
    "        plt.show()\n",
    "        \n",
    "    ensembled = np.vstack(y_preds).mean(axis=0)\n",
    "    title = f'Ensemble of {n_estimators} trees, with max_depth={max_depth}'\n",
    "    ax = putts.plot('distance', 'rate of success', kind='scatter', title=title)\n",
    "    ax.step(putts_X, ensembled, where='mid')\n",
    "    plt.show()\n",
    "    \n",
    "interact(diy_bagging, max_depth=(1,6,1), n_estimators=(2,5,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's \"random\" about random forests?\n",
    "1. Each tree trains on a random bootstrap sample of the data. (In scikit-learn, for `RandomForestRegressor` and `RandomForestClassifier`, the `bootstrap` parameter's default is `True`.) This type of ensembling is called Bagging.\n",
    "2. Each split considers a random subset of the features. (In scikit-learn, when the `max_features` parameter is not `None`.) \n",
    "\n",
    "For extra randomness, you can try [\"extremely randomized trees\"](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees)!\n",
    "\n",
    ">In extremely randomized trees (see [ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) and [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting \n",
    "\n",
    "Boosting (used by Gradient Boosting) is different than Bagging (used by Random Forests). \n",
    "\n",
    "[_An Introduction to Statistical Learning_](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf) Chapter 8.2.3, Boosting:\n",
    "\n",
    ">Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model.\n",
    "\n",
    ">**Boosting works in a similar way, except that the trees are grown _sequentially_: each tree is grown using information from previously grown trees.**\n",
    "\n",
    ">Unlike fitting a single large decision tree to the data, which amounts to _fitting the data hard_ and potentially overfitting, the boosting approach instead _learns slowly._ Given the current model, we fit a decision tree to the residuals from the model.\n",
    "\n",
    ">We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes. **By fitting small trees to the residuals, we slowly improve fˆ in areas where it does not perform well.**\n",
    "\n",
    ">Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wave (regression, 1 feature, non-monotonic, train/test split)\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_data():\n",
    "    import numpy as np\n",
    "    rng = np.random.RandomState(1)\n",
    "    X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "    y = np.sin(X).ravel()\n",
    "    y[::5] += 2 * (0.5 - rng.rand(16))\n",
    "    return X, y\n",
    "\n",
    "wave_X, wave_y = make_data()\n",
    "wave_X_train, wave_X_test, wave_y_train, wave_y_test = train_test_split(\n",
    "    wave_X, wave_y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wave_trees(max_depth=1, n_estimators=10):\n",
    "    models = [DecisionTreeRegressor(max_depth=max_depth), \n",
    "              RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators), \n",
    "              XGBRegressor(max_depth=max_depth, n_estimators=n_estimators)]\n",
    "    \n",
    "    for model in models:\n",
    "        name = model.__class__.__name__\n",
    "        model.fit(wave_X_train, wave_y_train)\n",
    "        print(f'{name} Train R^2 score:', model.score(wave_X_train, wave_y_train))\n",
    "        print(f'{name} Test R^2 score:', model.score(wave_X_test, wave_y_test))\n",
    "        plt.scatter(wave_X_train, wave_y_train)\n",
    "        plt.scatter(wave_X_test, wave_y_test)\n",
    "        plt.step(wave_X, model.predict(wave_X), where='mid')\n",
    "        plt.show()\n",
    "        \n",
    "interact(wave_trees, max_depth=(1,8,1), n_estimators=(10,40,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic (classification, 2 features, interactions, non-linear / non-monotonic)\n",
    "\n",
    "#### viz2D helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz2D(fitted_model, X, feature1, feature2, num=100, title=''):\n",
    "    \"\"\"\n",
    "    Visualize model predictions as a 2D heatmap\n",
    "    For regression or binary classification models, fitted on 2 features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fitted_model : scikit-learn model, already fitted\n",
    "    df : pandas dataframe, which was used to fit model\n",
    "    feature1 : string, name of feature 1\n",
    "    feature2 : string, name of feature 2\n",
    "    target : string, name of target\n",
    "    num : int, number of grid points for each feature\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictions: numpy array, predictions/predicted probabilities at each grid point\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    https://scikit-learn.org/stable/auto_examples/classification/plot_classification_probability.html\n",
    "    https://jakevdp.github.io/PythonDataScienceHandbook/04.04-density-and-contour-plots.html\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(X[feature1].min(), X[feature1].max(), num)\n",
    "    x2 = np.linspace(X[feature2].min(), X[feature2].max(), num)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    X = np.c_[X1.flatten(), X2.flatten()]\n",
    "    if hasattr(fitted_model, 'predict_proba'):\n",
    "        predicted = fitted_model.predict_proba(X)[:,1]\n",
    "    else:\n",
    "        predicted = fitted_model.predict(X)\n",
    "    \n",
    "    plt.imshow(predicted.reshape(num, num), cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data, encode categorical feature, impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "features = ['age', 'sex']\n",
    "target = 'survived'\n",
    "\n",
    "preprocessor = make_pipeline(ce.OrdinalEncoder(), SimpleImputer())\n",
    "X = preprocessor.fit_transform(titanic[features])\n",
    "X = pd.DataFrame(X, columns=features)\n",
    "y = titanic[target]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X, y)\n",
    "viz2D(lr, X, feature1='age', feature2='sex', title='Logistic Regression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree, Random Forest, Gradient Boosting\n",
    "\n",
    "#### Docs\n",
    "- [Scikit-Learn User Guide: Random Forests](https://scikit-learn.org/stable/modules/ensemble.html#random-forests) (`from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier`)\n",
    "- [XGBoost Python API Reference: Scikit-Learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn) (`from xgboost import XGBRegressor, XGBClassifier`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def titanic_trees(max_depth=1, n_estimators=1):\n",
    "    models = [DecisionTreeClassifier(max_depth=max_depth), \n",
    "              RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators), \n",
    "              XGBClassifier(max_depth=max_depth, n_estimators=n_estimators)]\n",
    "    \n",
    "    for model in models:\n",
    "        name = model.__class__.__name__\n",
    "        model.fit(X.values, y.values)\n",
    "        viz2D(model, X, feature1='age', feature2='sex', title=name)\n",
    "        \n",
    "interact(titanic_trees, max_depth=(1,6,1), n_estimators=(10,40,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do-it-yourself Bagging Ensemble of Decision Trees (like a Random Forest)\n",
    "\n",
    "def titanic_bagging(max_depth=1, n_estimators=1):\n",
    "    predicteds = []\n",
    "    for i in range(n_estimators):\n",
    "        title = f'Tree {i+1}'\n",
    "        bootstrap_sample = titanic.sample(n=len(titanic), replace=True)\n",
    "        preprocessor = make_pipeline(ce.OrdinalEncoder(), SimpleImputer())\n",
    "        bootstrap_X = preprocessor.fit_transform(bootstrap_sample[['age', 'sex']])\n",
    "        bootstrap_y = bootstrap_sample['survived']\n",
    "        tree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "        tree.fit(bootstrap_X, bootstrap_y)\n",
    "        predicted = viz2D(tree, X, feature1='age', feature2='sex', title=title)\n",
    "        predicteds.append(predicted)\n",
    "    \n",
    "    ensembled = np.vstack(predicteds).mean(axis=0)\n",
    "    title = f'Ensemble of {n_estimators} trees, with max_depth={max_depth}'\n",
    "    plt.imshow(ensembled.reshape(100, 100), cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('age')\n",
    "    plt.ylabel('sex')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "        \n",
    "interact(titanic_bagging, max_depth=(1,6,1), n_estimators=(2,5,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select more features, compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "titanic['deck'] = titanic['deck'].astype(str)\n",
    "features = ['age', 'sex', 'pclass', 'sibsp', 'parch', 'fare', 'deck', 'embark_town']\n",
    "target = 'survived'\n",
    "\n",
    "preprocessor = make_pipeline(ce.OrdinalEncoder(), SimpleImputer(), MinMaxScaler())\n",
    "titanic_X = preprocessor.fit_transform(titanic[features])\n",
    "titanic_X = pd.DataFrame(titanic_X, columns=features)\n",
    "titanic_y = titanic[target]\n",
    "\n",
    "titanic_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = [LogisticRegression(solver='lbfgs', max_iter=1000), \n",
    "          DecisionTreeClassifier(max_depth=3), \n",
    "          DecisionTreeClassifier(max_depth=None), \n",
    "          RandomForestClassifier(max_depth=3, n_estimators=100, n_jobs=-1, random_state=42), \n",
    "          RandomForestClassifier(max_depth=None, n_estimators=100, n_jobs=-1, random_state=42), \n",
    "          XGBClassifier(max_depth=3, n_estimators=100, n_jobs=-1, random_state=42)]\n",
    "\n",
    "for model in models:\n",
    "    print(model, '\\n')\n",
    "    score = cross_val_score(model, titanic_X, titanic_y, scoring='accuracy', cv=5).mean()\n",
    "    print('Cross-Validation Accuracy:', score, '\\n', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    name = model.__class__.__name__\n",
    "    model.fit(titanic_X, titanic_y)\n",
    "    if name == 'LogisticRegression':\n",
    "        coefficients = pd.Series(model.coef_[0], titanic_X.columns)\n",
    "        coefficients.sort_values().plot.barh(color='grey', title=name)\n",
    "        plt.show()\n",
    "    else:\n",
    "        importances = pd.Series(model.feature_importances_, titanic_X.columns)\n",
    "        title = f'{name}, max_depth={model.max_depth}'\n",
    "        importances.sort_values().plot.barh(color='grey', title=title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT\n",
    "\n",
    "**Train Random Forest and Gradient Boosting models**, on the Bank Marketing dataset. (Or another dataset of your choice, not used during this lesson.) You may use any Python libraries for Gradient Boosting.\n",
    "\n",
    "Then, you have many options!\n",
    "\n",
    "#### Keep improving your model\n",
    "- **Try new categorical encodings.**\n",
    "- Explore and visualize your data. \n",
    "- Wrangle [bad data](https://github.com/Quartz/bad-data-guide), outliers, and missing values.\n",
    "- Try engineering more features. You can transform, bin, and combine features. \n",
    "- Try selecting fewer features.\n",
    "\n",
    "#### Follow the links — learn by reading & doing\n",
    "- Links at the top of this notebook\n",
    "- Links in previous notebooks\n",
    "- Extra notebook for today, about **\"monotonic constraints\"** and \"early stopping\" with xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
